{"cells":[{"cell_type":"markdown","metadata":{"id":"CL495WoAR_Kw"},"source":["## PyTorch Datasets and DataLoaders"]},{"cell_type":"markdown","metadata":{"id":"Kw0liepaSDDT"},"source":["The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1682716390431,"user":{"displayName":"Zane Durante","userId":"05159585714773361430"},"user_tz":420},"id":"urI46YyRSL0I","outputId":"bdcdeaf3-bc52-4ace-a389-bbfbdd5ac600"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (2.2.2)\n","Requirement already satisfied: torchvision in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (0.17.2)\n","Requirement already satisfied: filelock in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from torch) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from torch) (2024.3.1)\n","Requirement already satisfied: numpy in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from torchvision) (1.26.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from torchvision) (10.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in g:\\我的雲端硬碟\\python\\.conda\\lib\\site-packages (from sympy->torch) (1.3.0)\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Num training examples: 50000\n","Num test examples: 10000\n"]}],"source":["!pip install torch torchvision\n","#!pip install transform\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# Define data transformations for training data\n","transform_train = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),  # 随机水平翻转图像\n","    transforms.RandomCrop(32, padding=4),  # 随机裁剪图像，并在周围填充0\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n","])\n","\n","# Define data transformations for test data\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n","])\n","\n","# Load the CIFAR10 dataset\n","train_dataset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train\n",")\n","\n","test_dataset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test\n",")\n","\n","# Create DataLoaders for train and test datasets\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=64, shuffle=True, num_workers=2\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=64, shuffle=False, num_workers=2\n",")\n","\n","print(\"Num training examples: {}\".format(len(train_dataset)))\n","print(\"Num test examples: {}\".format(len(test_dataset)))\n","\n","\n","# List of class labels\n","classes = [\n","    'plane', 'car', 'bird', 'cat',\n","    'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n","]"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([64, 3, 32, 32]),\n"," torch.float32,\n"," -1.0,\n"," 1.0,\n"," torch.Size([64]),\n"," torch.int64)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Fetch one batch of data\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","\n","# Inspect the shapes and value ranges of images (X) and labels (Y)\n","images_shape = images.shape\n","images_dtype = images.dtype\n","images_min = images.min().item()\n","images_max = images.max().item()\n","\n","labels_shape = labels.shape\n","labels_dtype = labels.dtype\n","\n","images_shape, images_dtype, images_min, images_max, labels_shape, labels_dtype"]},{"cell_type":"markdown","metadata":{"id":"F_G06d-GWvGw"},"source":["## Visualizing Examples from the CIFAR10 Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"elapsed":2150,"status":"ok","timestamp":1682716482681,"user":{"displayName":"Zane Durante","userId":"05159585714773361430"},"user_tz":420},"id":"WvIF1OOuWzqG","outputId":"3a82526a-a63c-4bff-f185-0c76cae1599a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7.2)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["%pip install matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Function to unnormalize and display an image\n","def imshow(img):\n","    img = img / 2 + 0.5  # Unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","# Get a batch of training data\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","\n","# Display the images in a grid along with their labels\n","imshow(torchvision.utils.make_grid(images[:16]))\n","print(\" -- \".join(f\"{classes[labels[j]]}\" for j in range(8)))\n","print(\" -- \".join(f\"{classes[labels[j]]}\" for j in range(8,16)))"]},{"cell_type":"markdown","metadata":{"id":"f_hfcVZsUqhg"},"source":["## Training on the GPU and Evaluating Performance\n","Calculate train and test accuracy"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# Set the random seed for reproducibility\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","\n","# Define the neural network\n","class TwoLayerNet(torch.nn.Module):\n","    def __init__(self, D_in, H, D_out):\n","        super(TwoLayerNet, self).__init__()\n","        self.linear1 = torch.nn.Linear(D_in, H)\n","        self.dropout = nn.Dropout(p=0.5)  # Add dropout layer with 50% dropout rate\n","        self.bn1 = nn.BatchNorm1d(H)  # Add batch normalization layer\n","        self.linear2 = torch.nn.Linear(H, D_out)\n","        self.device = 'cpu'\n","        self.use_dropout = False  # Flag to control dropout\n","        self.use_bn = False  # Flag to control batch normalization\n","\n","        # Initialize weights of self.linear with a small random number from a normal distribution\n","        #nn.init.normal_(self.linear1.weight, mean=0.0, std=0.01)\n","        #nn.init.normal_(self.linear2.weight, mean=0.0, std=0.01)\n","        nn.init.kaiming_uniform_(self.linear1.weight, mode='fan_in', nonlinearity='relu')\n","        nn.init.kaiming_uniform_(self.linear2.weight, mode='fan_in', nonlinearity='relu')\n","\n","    def forward(self, x):\n","        #x = x.view(-1, 3*32*32)  # 將x展平以匹配全連接層的期望輸入形狀\n","        #h_relu = self.linear1(x).clamp(min=0)\n","        #y_pred = self.linear2(h_relu)\n","        #return y_pred\n","        x = x.view(-1, 3*32*32)  # Flatten input to match the input shape expected by the fully connected layer\n","        h = self.linear1(x)\n","        if self.use_dropout:\n","            h = self.dropout(h)  # Apply dropout if flag is set\n","        if self.use_bn:\n","            h = self.bn1(h)  # Apply batch normalization if flag is set\n","        h_relu = h.clamp(min=0)\n","        y_pred = self.linear2(h_relu)\n","        return y_pred\n","    \n","    def to(self, device):\n","        self.device = device  # Set the device attribute\n","        super().to(device)    # Call the parent class' to method    \n","    \n","    def evaluate_model(self, test_loader, criterion):\n","        # let's evaluate its performance on the test dataset.\n","        self.eval()\n","        # Test the neural network\n","        correct = 0\n","        total = 0\n","        test_loss = 0.0\n","\n","        # Disable gradient calculation\n","        with torch.no_grad():\n","            for inputs, labels in test_loader:\n","                #print(inputs.size(), labels.size())\n","\n","                # Move the inputs and labels to the GPU if available\n","                inputs = inputs.to(self.device)\n","                labels = labels.to(self.device)\n","\n","                # Forward pass\n","                outputs = self(inputs)\n","\n","                # Calculate test loss\n","                loss = criterion(outputs, labels)\n","                test_loss += loss.item() * inputs.size(0)  # 将损失累加，考虑到每个批次的大小\n","                \n","                # Get the predicted class\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Update the total number of samples and correct predictions\n","                total += labels.size(0) \n","                correct += (predicted == labels).sum().item()\n","\n","        # 计算平均测试损失\n","        avg_test_loss = test_loss / len(test_loader.dataset)\n","        # Calculate the accuracy\n","        test_accuracy = 100 * correct / total\n","        return avg_test_loss, test_accuracy"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def reset_and_initialize():\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Using device:\", device)\n","    net = TwoLayerNet(3*32*32, 1000, 10)\n","    net.to(device)\n","    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.001)#weight_decay=L2λ 添加一个与权重大小相关的L2项来抑制过拟合，影响梯度更新。\n","    return net, optimizer, device"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def custom_loss(outputs, targets, model, lambda1):\n","    criterion = nn.CrossEntropyLoss()\n","    data_loss = criterion(outputs, targets)\n","    reg_loss = 0\n","    p = model.linear1.out_features  # Number of hidden nodes\n","    m = model.linear1.in_features  # Number of input nodes\n","    for param in model.parameters():\n","        reg_loss += torch.sum(param**2)\n","    total_loss = data_loss + (lambda1 / (p + 1 + p * (m + 1))) * reg_loss\n","    return total_loss"]},{"cell_type":"markdown","metadata":{},"source":["### tuning_EB + regularizing_EB"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87295,"status":"ok","timestamp":1682717719007,"user":{"displayName":"Zane Durante","userId":"05159585714773361430"},"user_tz":420},"id":"n4F3cP8tUelq","outputId":"a4cd6798-15c3-4a8e-8fa9-baed1306d3b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Epoch: 1, TrainLoss: 1.9404, TrainAccuracy: 34.05%, TestLoss: 2.0453, TestAccuracy: 37.34%\n","Epoch: 2, TrainLoss: 1.7097, TrainAccuracy: 39.23%, TestLoss: 1.7969, TestAccuracy: 39.69%\n","Epoch: 3, TrainLoss: 1.6668, TrainAccuracy: 40.49%, TestLoss: 1.9671, TestAccuracy: 38.84%\n","Epoch: 4, TrainLoss: 1.6542, TrainAccuracy: 40.85%, TestLoss: 2.2143, TestAccuracy: 36.60%\n","Epoch: 5, TrainLoss: 1.6533, TrainAccuracy: 41.17%, TestLoss: 2.0067, TestAccuracy: 38.29%\n","Training phase completes.\n","Regularization phase starts.\n","Epoch: 1, TrainLoss: 1.6367, TrainAccuracy: 41.50%, TestLoss: 1.7896, TestAccuracy: 40.54%\n","Epoch: 2, TrainLoss: 1.6314, TrainAccuracy: 41.90%, TestLoss: 1.8771, TestAccuracy: 39.79%\n","Epoch: 3, TrainLoss: 1.6322, TrainAccuracy: 41.86%, TestLoss: 1.8855, TestAccuracy: 40.14%\n","Epoch: 4, TrainLoss: 1.6219, TrainAccuracy: 42.18%, TestLoss: 1.7788, TestAccuracy: 41.61%\n","Epoch: 5, TrainLoss: 1.6213, TrainAccuracy: 42.12%, TestLoss: 1.8081, TestAccuracy: 42.56%\n","Stopping regularizing in the epoch 6 as new training did not converge, the average train loss 1.6213 is larger than epsilon 0.5, acceptable 2LNN but using old weights.\n"]}],"source":["# Example usage\n","net, optimizer, device = reset_and_initialize()\n","criterion = nn.CrossEntropyLoss() #分類任務的損失函數\n","\n","trn_loss_eb = []\n","trn_acc_eb = []\n","val_loss_eb = []\n","val_acc_eb = []\n","\n","trn_loss_eb_r = []\n","trn_acc_eb_r = []\n","val_loss_eb_r = []\n","val_acc_eb_r = [] \n","\n","# Train the neural network\n","max_epoch = 5\n","epoch_eb_r = 0\n","max_epoch_r = 5\n","epsilon_r = 0.5\n","\n","for epoch_eb in range(max_epoch):\n","    # Set the model to training mode\n","    net.train()  \n","    running_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for i, (inputs, labels) in enumerate(train_loader, 0): #每個epoch内，模型處理782個mini-batch=64張圖，共50000張圖。\n","        # Move the inputs and labels to the GPU if available\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()# Zero the gradients\n","        outputs = net(inputs)# Forward pass\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update the running loss\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","\n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss = running_loss / (i + 1)\n","    train_accuracy = 100 * train_correct / train_total\n","    trn_loss_eb.append(avg_train_loss)\n","    trn_acc_eb.append(train_accuracy)\n","\n","    # Evaluate on test data after each epoch\n","    avg_test_loss, test_accuracy = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb + 1}, TrainLoss: {avg_train_loss:.4f}, TrainAccuracy: {train_accuracy:.2f}%, TestLoss: {avg_test_loss:.4f}, TestAccuracy: {test_accuracy:.2f}%\")\n","    val_loss_eb.append(avg_test_loss)\n","    val_acc_eb.append(test_accuracy)\n","\n","print('Training phase completes.')\n","# Save the acceptable state\n","torch.save(net.state_dict(), 'acceptable_2lnn.pth')\n","\n","# Load weights if needed and begin the regularization phase\n","net.load_state_dict(torch.load('acceptable_2lnn.pth'))\n","print(\"Regularization phase starts.\")\n","\n","# Run a loop with custom loss that includes regularization\n","while epoch_eb_r < max_epoch_r:\n","    net.train()\n","    running_loss_r = 0.0\n","    train_correct_r = 0\n","    train_total_r = 0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = custom_loss(outputs, labels, net, 0.001)  # Use the custom loss function\n","        loss.backward(retain_graph=True)\n","        optimizer.step()\n","        \n","        running_loss_r += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # Update the running loss\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total_r += labels.size(0)\n","        train_correct_r += (predicted == labels).sum().item()\n","\n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss_r = running_loss_r / (i + 1)\n","    train_accuracy_r = 100 * train_correct_r / train_total_r\n","    trn_loss_eb_r.append(avg_train_loss_r)\n","    trn_acc_eb_r.append(train_accuracy_r)\n","\n","    # Evaluate on test data after each epoch\n","    avg_test_loss_r, test_accuracy_r = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb_r + 1}, TrainLoss: {avg_train_loss_r:.4f}, TrainAccuracy: {train_accuracy_r:.2f}%, TestLoss: {avg_test_loss_r:.4f}, TestAccuracy: {test_accuracy_r:.2f}%\")\n","    val_loss_eb_r.append(avg_test_loss_r)\n","    val_acc_eb_r.append(test_accuracy_r)\n","    epoch_eb_r += 1\n","\n","if avg_train_loss_r < epsilon_r:\n","    print(f\"Stopping regularizing in the epoch {epoch_eb_r+1} as the average train loss {avg_train_loss_r:.4f} is less than epsilon {epsilon_r}, acceptable 2LNN with new weights.\")\n","elif avg_train_loss_r >= epsilon_r:\n","    print(f\"Stopping regularizing in the epoch {epoch_eb_r+1} as new training did not converge, the average train loss {avg_train_loss_r:.4f} is larger than epsilon {epsilon_r}, acceptable 2LNN but using old weights.\")\n","    net.load_state_dict(torch.load('acceptable_2lnn.pth'))\n"]},{"cell_type":"markdown","metadata":{},"source":["### tuning_EB + regularizing_BN"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Epoch: 1, TrainLoss: 1.9511, TrainAccuracy: 33.80%, TestLoss: 2.4108, TestAccuracy: 35.26%\n","Epoch: 2, TrainLoss: 1.7178, TrainAccuracy: 38.98%, TestLoss: 1.8659, TestAccuracy: 39.09%\n","Training phase completes.\n","Regularization phase starts.\n","Epoch: 1, TrainLoss: 1.7504, TrainAccuracy: 37.39%, TestLoss: 1.6336, TestAccuracy: 41.54%\n","Epoch: 2, TrainLoss: 1.6907, TrainAccuracy: 39.61%, TestLoss: 1.5730, TestAccuracy: 44.39%\n","Stopping regularizing in the epoch 3 as new training did not converge, the average train loss 1.6907 is larger than epsilon 0.5, acceptable 2LNN but using old weights.\n"]}],"source":["# Example usage\n","net, optimizer, device = reset_and_initialize()\n","criterion = nn.CrossEntropyLoss() #分類任務的損失函數\n","\n","trn_loss_eb = []\n","trn_acc_eb = []\n","val_loss_eb = []\n","val_acc_eb = []\n","\n","trn_loss_eb_r_bn = []\n","trn_acc_eb_r_bn = []\n","val_loss_eb_r_bn = []\n","val_acc_eb_r_bn = [] \n","\n","# Train the neural network\n","max_epoch = 2\n","epoch_eb_r_bn = 0\n","max_epoch_r_bn = 2\n","epsilon_r_bn = 0.5\n","\n","for epoch_eb in range(max_epoch):\n","    # Set the model to training mode\n","    net.train()  \n","    running_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for i, (inputs, labels) in enumerate(train_loader, 0): #每個epoch内，模型處理782個mini-batch=64張圖，共50000張圖。\n","        # Move the inputs and labels to the GPU if available\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()# Zero the gradients\n","        outputs = net(inputs)# Forward pass\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update the running loss\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","\n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss = running_loss / (i + 1)\n","    train_accuracy = 100 * train_correct / train_total\n","    trn_loss_eb.append(avg_train_loss)\n","    trn_acc_eb.append(train_accuracy)\n","\n","    # Evaluate on test data after each epoch\n","    avg_test_loss, test_accuracy = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb + 1}, TrainLoss: {avg_train_loss:.4f}, TrainAccuracy: {train_accuracy:.2f}%, TestLoss: {avg_test_loss:.4f}, TestAccuracy: {test_accuracy:.2f}%\")\n","    val_loss_eb.append(avg_test_loss)\n","    val_acc_eb.append(test_accuracy)\n","\n","print('Training phase completes.')\n","# Save the acceptable state\n","torch.save(net.state_dict(), 'acceptable_2lnn.pth')\n","\n","# Load weights if needed and begin the regularization phase\n","net.load_state_dict(torch.load('acceptable_2lnn.pth'))\n","net.use_bn = True  # Enable bn for regularization phase\n","print(\"Regularization phase starts.\")\n","\n","# Run a loop with custom loss that includes regularization\n","while epoch_eb_r_bn < max_epoch_r_bn:\n","    net.train()\n","    running_loss_r_bn = 0.0\n","    train_correct_r_bn = 0\n","    train_total_r_bn = 0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss_r_bn = criterion(outputs, labels)  # Use the custom loss function\n","        loss_r_bn.backward(retain_graph=True)\n","        optimizer.step()\n","        \n","        # Update the running loss\n","        running_loss_r_bn += loss_r_bn.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total_r_bn += labels.size(0)\n","        train_correct_r_bn += (predicted == labels).sum().item()\n","\n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss_r_bn = running_loss_r_bn / (i + 1)\n","    train_accuracy_r_bn = 100 * train_correct_r_bn / train_total_r_bn\n","    trn_loss_eb_r_bn.append(avg_train_loss_r_bn)\n","    trn_acc_eb_r_bn.append(train_accuracy_r_bn)\n","\n","    # Evaluate on test data after each epoch\n","    avg_test_loss_r_bn, test_accuracy_r_bn = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb_r_bn + 1}, TrainLoss: {avg_train_loss_r_bn:.4f}, TrainAccuracy: {train_accuracy_r_bn:.2f}%, TestLoss: {avg_test_loss_r_bn:.4f}, TestAccuracy: {test_accuracy_r_bn:.2f}%\")\n","    val_loss_eb_r_bn.append(avg_test_loss_r_bn)\n","    val_acc_eb_r_bn.append(test_accuracy_r_bn)\n","    epoch_eb_r_bn += 1\n","\n","if avg_train_loss_r_bn < epsilon_r_bn:\n","    print(f\"Stopping regularizing in the epoch {epoch_eb_r_bn+1} as the average train loss {avg_train_loss_r_bn:.4f} is less than epsilon {epsilon_r_bn}, acceptable 2LNN with new weights.\")\n","elif avg_train_loss_r_bn >= epsilon_r_bn:\n","    net.load_state_dict(torch.load('acceptable_2lnn.pth'))\n","    print(f\"Stopping regularizing in the epoch {epoch_eb_r_bn+1} as new training did not converge, the average train loss {avg_train_loss_r_bn:.4f} is larger than epsilon {epsilon_r_bn}, acceptable 2LNN but using old weights.\")\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["### tuning_EB + regularizing_DO"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]}],"source":["# Example usage\n","net, optimizer, device = reset_and_initialize()\n","criterion = nn.CrossEntropyLoss() #分類任務的損失函數\n","\n","trn_loss_eb = []\n","trn_acc_eb = []\n","val_loss_eb = []\n","val_acc_eb = []\n","\n","trn_loss_eb_r_dr = []\n","trn_acc_eb_r_dr = []\n","val_loss_eb_r_dr = []\n","val_acc_eb_r_dr = [] \n","\n","# Train the neural network\n","max_epoch = 2\n","epoch_eb_r_dr = 0\n","max_epoch_r_dr = 2\n","epsilon_r_dr = 0.5\n","\n","for epoch_eb in range(max_epoch):\n","    # Set the model to training mode\n","    net.train()  \n","    running_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for i, (inputs, labels) in enumerate(train_loader, 0): #每個epoch内，模型處理782個mini-batch=64張圖，共50000張圖。\n","        # Move the inputs and labels to the GPU if available\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()# Zero the gradients\n","        outputs = net(inputs)# Forward pass\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update the running loss\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","\n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss = running_loss / (i + 1)\n","    train_accuracy = 100 * train_correct / train_total\n","    trn_loss_eb.append(avg_train_loss)\n","    trn_acc_eb.append(train_accuracy)\n","\n","    # Evaluate on test data after each epoch\n","    avg_test_loss, test_accuracy = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb + 1}, TrainLoss: {avg_train_loss:.4f}, TrainAccuracy: {train_accuracy:.2f}%, TestLoss: {avg_test_loss:.4f}, TestAccuracy: {test_accuracy:.2f}%\")\n","    val_loss_eb.append(avg_test_loss)\n","    val_acc_eb.append(test_accuracy)\n","\n","print('Training phase completes.')\n","# Save the acceptable state\n","torch.save(net.state_dict(), 'acceptable_2lnn.pth')\n","\n","# Load weights if needed and begin the regularization phase\n","net.load_state_dict(torch.load('acceptable_2lnn.pth'))\n","net.use_dropout = True  # Enable dropout for regularization phase\n","print(\"Regularization phase starts.\")\n","\n","# Run a loop with custom loss that includes regularization\n","while epoch_eb_r_dr < max_epoch_r_dr:\n","    net.train()\n","    running_loss_r_dr = 0.0\n","    train_correct_r_dr = 0\n","    train_total_r_dr = 0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss_r_dr = criterion(outputs, labels)  # Use the custom loss function\n","        loss_r_dr.backward(retain_graph=True)\n","        optimizer.step()\n","        \n","        running_loss_r_dr += loss_r_dr.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total_r_dr += labels.size(0)\n","        train_correct_r_dr += (predicted == labels).sum().item()\n","\n","\n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss_r_dr = running_loss_r_dr / (i + 1)\n","    train_accuracy_r_dr = 100 * train_correct_r_dr / train_total_r_dr\n","    trn_loss_eb_r_dr.append(avg_train_loss_r_dr)\n","    trn_acc_eb_r_dr.append(train_accuracy_r_dr)\n","\n","    # Evaluate on test data after each epoch\n","    avg_test_loss_r_dr, test_accuracy_r_dr = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb_r_dr + 1}, TrainLoss: {avg_train_loss_r_dr:.4f}, TrainAccuracy: {train_accuracy_r_dr:.2f}%, TestLoss: {avg_test_loss_r_dr:.4f}, TestAccuracy: {test_accuracy_r_dr:.2f}%\")\n","    val_loss_eb_r_dr.append(avg_test_loss_r_dr)\n","    val_acc_eb_r_dr.append(test_accuracy_r_dr)\n","    epoch_eb_r_dr += 1\n","\n","if avg_train_loss_r_dr < epsilon_r_dr:\n","    print(f\"Stopping regularizing in the epoch {epoch_eb_r_dr+1} as the average train loss {avg_train_loss_r_dr:.4f} is less than epsilon {epsilon_r_dr}, acceptable 2LNN with new weights.\")\n","elif avg_train_loss_r_dr >= epsilon_r_bn:\n","    net.load_state_dict(torch.load('acceptable_2lnn.pth'))\n","    print(f\"Stopping regularizing in the epoch {epoch_eb_r_dr+1} as new training did not converge, the average train loss {avg_train_loss_r_dr:.4f} is larger than epsilon {epsilon_r_dr}, acceptable 2LNN but using old weights.\")\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["### tuning_LG"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Epoch: 1, TrainLoss: 1.9448, TrainAccuracy: 33.51%, TestLoss: 2.4100, TestAccuracy: 35.24%\n","Epoch: 2, TrainLoss: 1.7156, TrainAccuracy: 39.30%, TestLoss: 1.8786, TestAccuracy: 38.38%\n","Epoch: 3, TrainLoss: 1.6748, TrainAccuracy: 39.98%, TestLoss: 1.8491, TestAccuracy: 40.63%\n","Epoch: 4, TrainLoss: 1.6597, TrainAccuracy: 40.90%, TestLoss: 1.7286, TestAccuracy: 41.25%\n","Epoch: 5, TrainLoss: 1.6513, TrainAccuracy: 41.32%, TestLoss: 1.9521, TestAccuracy: 37.82%\n","Epoch: 6, TrainLoss: 1.6431, TrainAccuracy: 41.82%, TestLoss: 1.7475, TestAccuracy: 40.61%\n","Epoch: 7, TrainLoss: 1.6301, TrainAccuracy: 41.66%, TestLoss: 1.9061, TestAccuracy: 38.77%\n","Epoch: 8, TrainLoss: 1.6250, TrainAccuracy: 42.25%, TestLoss: 1.8637, TestAccuracy: 40.69%\n","Epoch: 9, TrainLoss: 1.6263, TrainAccuracy: 41.88%, TestLoss: 1.8138, TestAccuracy: 41.20%\n","Epoch: 10, TrainLoss: 1.6235, TrainAccuracy: 42.31%, TestLoss: 1.9024, TestAccuracy: 39.39%\n","Epoch: 11, TrainLoss: 1.6195, TrainAccuracy: 42.33%, TestLoss: 1.8763, TestAccuracy: 40.93%\n","Epoch: 12, TrainLoss: 1.6112, TrainAccuracy: 42.55%, TestLoss: 1.8057, TestAccuracy: 41.55%\n","Epoch: 13, TrainLoss: 1.6150, TrainAccuracy: 42.49%, TestLoss: 1.8332, TestAccuracy: 40.55%\n","Epoch: 14, TrainLoss: 1.6105, TrainAccuracy: 42.52%, TestLoss: 2.1352, TestAccuracy: 39.14%\n","Epoch: 15, TrainLoss: 1.6073, TrainAccuracy: 42.75%, TestLoss: 1.8472, TestAccuracy: 38.78%\n","Epoch: 16, TrainLoss: 1.6062, TrainAccuracy: 42.94%, TestLoss: 1.7488, TestAccuracy: 40.80%\n","Epoch: 17, TrainLoss: 1.6082, TrainAccuracy: 42.66%, TestLoss: 1.8334, TestAccuracy: 40.48%\n","Epoch: 18, TrainLoss: 1.6107, TrainAccuracy: 42.42%, TestLoss: 1.9168, TestAccuracy: 40.75%\n","Epoch: 19, TrainLoss: 1.6080, TrainAccuracy: 42.91%, TestLoss: 1.8187, TestAccuracy: 41.31%\n","Epoch: 20, TrainLoss: 1.6060, TrainAccuracy: 42.56%, TestLoss: 1.7277, TestAccuracy: 42.37%\n","Epoch: 21, TrainLoss: 1.6101, TrainAccuracy: 42.60%, TestLoss: 1.8714, TestAccuracy: 40.37%\n","Epoch: 22, TrainLoss: 1.6020, TrainAccuracy: 43.03%, TestLoss: 1.8048, TestAccuracy: 40.74%\n","Epoch: 23, TrainLoss: 1.6038, TrainAccuracy: 43.03%, TestLoss: 1.9027, TestAccuracy: 41.24%\n","Epoch: 24, TrainLoss: 1.6051, TrainAccuracy: 42.91%, TestLoss: 1.7951, TestAccuracy: 40.68%\n","Epoch: 25, TrainLoss: 1.6007, TrainAccuracy: 42.75%, TestLoss: 1.9337, TestAccuracy: 40.62%\n","Epoch: 26, TrainLoss: 1.5998, TrainAccuracy: 42.88%, TestLoss: 1.8864, TestAccuracy: 41.51%\n","Stopping training in the epoch 26 as the average train loss 1.5998 is less than epsilon 1.6, acceptable 2LNN.\n"]}],"source":["# Example usage\n","net, optimizer, device = reset_and_initialize()\n","criterion = nn.CrossEntropyLoss() #分類任務的損失函數\n","\n","trn_loss_lg = []\n","trn_acc_lg = []\n","val_loss_lg = []\n","val_acc_lg = []\n","\n","epsilon = 1.6\n","epoch_lg = 0\n","\n","while True:\n","    # Set the model to training mode\n","    net.train()  \n","    running_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for i, (inputs, labels) in enumerate(train_loader, 0): #每個epoch内，模型處理782個mini-batch=64張圖，共50000張圖。\n","        # Move the inputs and labels to the GPU if available\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = net(inputs)\n","\n","        # Compute the loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update the running loss\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","\n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss = running_loss / (i + 1)\n","    train_accuracy = 100 * train_correct / train_total\n","    trn_loss_lg.append(avg_train_loss)\n","    trn_acc_lg.append(train_accuracy)\n","\n","    # Evaluate on test data after each epoch\n","    avg_test_loss, test_accuracy = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_lg+1}, TrainLoss: {avg_train_loss:.4f}, TrainAccuracy: {train_accuracy:.2f}%, TestLoss: {avg_test_loss:.4f}, TestAccuracy: {test_accuracy:.2f}%\")\n","    val_loss_lg.append(avg_test_loss)\n","    val_acc_lg.append(test_accuracy)\n","    \n","    if avg_train_loss < epsilon:\n","        print(f\"Stopping training in the epoch {epoch_lg+1} as the average train loss {avg_train_loss:.4f} is less than epsilon {epsilon}, acceptable 2LNN.\")\n","        break\n","    epoch_lg += 1"]},{"cell_type":"markdown","metadata":{},"source":["### tuning_EB_LG"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Epoch: 1, TrainLoss: 1.9384, TrainAccuracy: 33.87%, TestLoss: 2.4364, TestAccuracy: 35.99%\n","Epoch: 2, TrainLoss: 1.7188, TrainAccuracy: 38.95%, TestLoss: 2.1307, TestAccuracy: 36.13%\n","Epoch: 3, TrainLoss: 1.6733, TrainAccuracy: 39.99%, TestLoss: 1.8197, TestAccuracy: 40.57%\n","Epoch: 4, TrainLoss: 1.6526, TrainAccuracy: 41.25%, TestLoss: 1.6833, TestAccuracy: 43.32%\n","Epoch: 5, TrainLoss: 1.6448, TrainAccuracy: 41.47%, TestLoss: 1.7939, TestAccuracy: 40.14%\n","Epoch: 6, TrainLoss: 1.6366, TrainAccuracy: 41.87%, TestLoss: 1.8496, TestAccuracy: 41.89%\n","Epoch: 7, TrainLoss: 1.6332, TrainAccuracy: 41.77%, TestLoss: 1.9389, TestAccuracy: 39.18%\n","Epoch: 8, TrainLoss: 1.6240, TrainAccuracy: 42.17%, TestLoss: 2.0265, TestAccuracy: 37.37%\n","Epoch: 9, TrainLoss: 1.6205, TrainAccuracy: 42.29%, TestLoss: 2.4311, TestAccuracy: 37.35%\n","Epoch: 10, TrainLoss: 1.6192, TrainAccuracy: 42.18%, TestLoss: 1.9803, TestAccuracy: 40.55%\n","Epoch: 11, TrainLoss: 1.6117, TrainAccuracy: 42.51%, TestLoss: 1.6993, TestAccuracy: 43.08%\n","Stopping training as the epoch 11 is larger than max epochs 10 , unacceptable 2LNN.\n"]}],"source":["# Example usage\n","net, optimizer, device = reset_and_initialize()\n","criterion = nn.CrossEntropyLoss() #分類任務的損失函數\n","\n","trn_loss_eb_lg = []\n","trn_acc_eb_lg = []\n","val_loss_eb_lg = []\n","val_acc_eb_lg = []  \n","\n","# Train the neural network\n","epsilon = 1.6\n","max_epochs = 10\n","epoch_eb_lg = 0\n","\n","#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n","while True:\n","    # Set the model to training mode\n","    net.train()  \n","    running_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for i, (inputs, labels) in enumerate(train_loader, 0): #每個epoch内，模型處理782個mini-batch=64張圖，共50000張圖。\n","        # Move the inputs and labels to the GPU if available\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = net(inputs)\n","\n","        # Compute the loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update the running loss\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","\n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss = running_loss / (i + 1)\n","    train_accuracy = 100 * train_correct / train_total\n","    trn_loss_eb_lg.append(avg_train_loss)\n","    trn_acc_eb_lg.append(train_accuracy)\n","\n","    # Evaluate on test data after each epoch\n","    avg_test_loss, test_accuracy = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb_lg+1}, TrainLoss: {avg_train_loss:.4f}, TrainAccuracy: {train_accuracy:.2f}%, TestLoss: {avg_test_loss:.4f}, TestAccuracy: {test_accuracy:.2f}%\")\n","    val_loss_eb_lg.append(avg_test_loss)\n","    val_acc_eb_lg.append(test_accuracy)\n","\n","    if avg_train_loss < epsilon:\n","        print(f\"Stopping training in the epoch {epoch_eb_lg+1} as the average train loss {avg_train_loss:.4f} is less than epsilon {epsilon}, acceptable 2LNN yes~\")\n","        break\n","    elif epoch_eb_lg >= max_epochs:\n","        print(f\"Stopping training as the epoch {epoch_eb_lg+1} is larger than max epochs {max_epochs} , unacceptable 2LNN.\")\n","        break\n","\n","    epoch_eb_lg += 1"]},{"cell_type":"markdown","metadata":{},"source":["### tuning_LG_UA + regularizing_LG_UA"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Epoch: 1, lr=0.001, TrainLoss: 1.9383, TrainAccuracy: 33.72%, TestLoss: 2.4334, TestAccuracy: 37.46%\n","Epoch: 2, lr=0.0012, TrainLoss: 1.7462, TrainAccuracy: 38.12%, TestLoss: 1.9765, TestAccuracy: 38.60%\n","Stopping training in the epoch 2 as the average train loss 1.7462 is less than epsilon 1.8, acceptable 2LNN.\n","Training phase completes.\n","Regularization phase starts.\n","Epoch: 1, lr=0.001, TrainLoss: 1.6973, TrainAccuracy: 39.15%, TestLoss: 1.8590, TestAccuracy: 39.38%\n","Epoch: 2, lr=0.0012, TrainLoss: 1.6859, TrainAccuracy: 39.91%, TestLoss: 1.8314, TestAccuracy: 38.33%\n","Epoch: 3, lr=0.0014399999999999999, TrainLoss: 1.7290, TrainAccuracy: 39.13%, TestLoss: 2.3120, TestAccuracy: 36.62%\n","Epoch: 4, lr=0.0010079999999999998, TrainLoss: 1.6363, TrainAccuracy: 41.44%, TestLoss: 1.8594, TestAccuracy: 39.97%\n","Epoch: 5, lr=0.0012095999999999997, TrainLoss: 1.6606, TrainAccuracy: 40.84%, TestLoss: 1.9479, TestAccuracy: 39.82%\n","Epoch: 6, lr=0.0008467199999999997, TrainLoss: 1.5986, TrainAccuracy: 42.73%, TestLoss: 1.5885, TestAccuracy: 44.31%\n","Epoch: 7, lr=0.0010160639999999997, TrainLoss: 1.6259, TrainAccuracy: 41.86%, TestLoss: 1.9429, TestAccuracy: 37.30%\n","Stopping regularizing in the epoch 7 as the eata 0.0007112447999999997 is smaller than epsilon_eta 0.0008, acceptable 2LNN.\n"]}],"source":["# Example usage\n","net, optimizer, device = reset_and_initialize()\n","criterion = nn.CrossEntropyLoss() #分類任務的損失函數\n","\n","trn_loss_lg_ua = []\n","trn_acc_lg_ua = []\n","val_loss_lg_ua = []\n","val_acc_lg_ua = []\n","\n","trn_loss_lg_ua_r = []\n","trn_acc_lg_ua_r = []\n","val_loss_lg_ua_r = []\n","val_acc_lg_ua_r = [] \n","\n","# Train the neural network\n","eta = 0.001\n","epsilon = 1.8\n","epsilon_eta = 0.00001\n","last_loss = float('inf')\n","epoch_lg_ua = 0\n","\n","eta_r = 0.001\n","epsilon_r = 1.72\n","epsilon_eta_r = 0.0008\n","last_loss_r = float('inf')\n","epoch_lg_ua_r = 0\n","\n","while True:\n","    net.train()\n","    running_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","                \n","    ## 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss = running_loss / len(train_loader) #len(train_loader)\n","    train_accuracy = 100 * train_correct / train_total\n","    #trn_loss_lg_ua.append(avg_train_loss)\n","    #trn_acc_lg_ua.append(train_accuracy)\n","\n","    ## 測試損失\n","    avg_test_loss, test_accuracy = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_lg_ua+1}, lr={eta}, TrainLoss: {avg_train_loss:.4f}, TrainAccuracy: {train_accuracy:.2f}%, TestLoss: {avg_test_loss:.4f}, TestAccuracy: {test_accuracy:.2f}%\")\n","    #val_loss_lg_ua.append(avg_test_loss)\n","    #val_acc_lg_ua.append(test_accuracy)\n","\n","    if avg_train_loss < epsilon:\n","        print(f\"Stopping training in the epoch {epoch_lg_ua+1} as the average train loss {avg_train_loss:.4f} is less than epsilon {epsilon}, acceptable 2LNN.\")\n","        break\n","    elif avg_train_loss < last_loss:\n","        eta *= 1.2\n","    else:\n","        eta *= 0.7\n","        if eta < epsilon_eta:\n","            print(f\"Stopping training in the epoch {epoch_lg_ua+1} as the eata {eta} is smaller than epsilon_eta {epsilon_eta}, unacceptable 2LNN.\")\n","            break\n","\n","    # Update the optimizer's learning rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = eta\n","\n","    last_loss = avg_train_loss\n","    epoch_lg_ua += 1\n","\n","print('Training phase completes.')\n","# Save the acceptable state\n","torch.save(net.state_dict(), 'acceptable_2lnn.pth')\n","\n","# Load weights if needed and begin the regularization phase\n","net.load_state_dict(torch.load('acceptable_2lnn.pth'))\n","print(\"Regularization phase starts.\")\n","\n","# 保存初始正則化階段的最佳權重\n","best_weights_r = net.state_dict().copy()\n","\n","# Run a loop with custom loss that includes regularization\n","while True:\n","    net.train()\n","    running_loss_r = 0.0\n","    train_correct_r = 0\n","    train_total_r = 0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = custom_loss(outputs, labels, net, 0.001)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss_r += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total_r += labels.size(0)\n","        train_correct_r += (predicted == labels).sum().item()\n","                \n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss_r = running_loss_r/ len(train_loader) #len(train_loader)\n","    train_accuracy_r = 100 * train_correct_r / train_total_r\n","    trn_loss_lg_ua_r.append(avg_train_loss_r)\n","    trn_acc_lg_ua_r.append(train_accuracy_r)\n","\n","    # 測試損失\n","    avg_test_loss_r, test_accuracy_r = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_lg_ua_r+1}, lr={eta_r}, TrainLoss: {avg_train_loss_r:.4f}, TrainAccuracy: {train_accuracy_r:.2f}%, TestLoss: {avg_test_loss_r:.4f}, TestAccuracy: {test_accuracy_r:.2f}%\")\n","    val_loss_lg_ua_r.append(avg_test_loss_r)\n","    val_acc_lg_ua_r.append(test_accuracy_r)\n","    \n","    if avg_train_loss_r <= last_loss_r:\n","        if avg_train_loss_r < epsilon_r:#@@\n","            eta_r *= 1.2\n","            best_weights_r = net.state_dict().copy()\n","        else:\n","            # 恢復上次的最佳權重\n","            net.load_state_dict(best_weights_r)\n","            print(f\"Stopping regularizing in the epoch {epoch_lg_ua_r+1} as the average train loss {avg_train_loss_r:.4f} is larger than epsilon {epsilon_r}, acceptable 2LNN.\") #大部分情況下是新的網路權重\n","            break\n","        # 保存當前最佳權重\n","\n","    else:\n","        eta_r *= 0.7\n","        if eta_r < epsilon_eta_r:\n","            net.load_state_dict(best_weights_r)\n","            print(f\"Stopping regularizing in the epoch {epoch_lg_ua_r+1} as the eata {eta_r} is smaller than epsilon_eta {epsilon_eta_r}, acceptable 2LNN.\")\n","            break\n","\n","    # Update the optimizer's learning rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = eta_r\n","\n","    last_loss_r = avg_train_loss_r\n","    epoch_lg_ua_r += 1"]},{"cell_type":"markdown","metadata":{},"source":["### tuning_EB_LG_UA + regularizing_EB_LG_UA"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000016E4F3EC160>\n","Traceback (most recent call last):\n","  File \"g:\\我的雲端硬碟\\Python\\.conda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","  File \"g:\\我的雲端硬碟\\Python\\.conda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1437, in _shutdown_workers\n","    if self._persistent_workers or self._workers_status[worker_id]:\n","AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"]},{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Epoch: 1, lr=0.001, TrainLoss: 1.9401, TrainAccuracy: 33.76%, TestLoss: 2.0604, TestAccuracy: 37.25%\n","Epoch: 2, lr=0.0012, TrainLoss: 1.7429, TrainAccuracy: 38.19%, TestLoss: 1.8562, TestAccuracy: 39.42%\n","Epoch: 3, lr=0.0014399999999999999, TrainLoss: 1.7337, TrainAccuracy: 38.42%, TestLoss: 2.1688, TestAccuracy: 36.86%\n","Epoch: 4, lr=0.0017279999999999997, TrainLoss: 1.7737, TrainAccuracy: 38.10%, TestLoss: 2.3518, TestAccuracy: 35.89%\n","Epoch: 5, lr=0.0012095999999999997, TrainLoss: 1.6747, TrainAccuracy: 40.19%, TestLoss: 1.8453, TestAccuracy: 40.47%\n","Epoch: 6, lr=0.0014515199999999995, TrainLoss: 1.7158, TrainAccuracy: 39.36%, TestLoss: 2.1137, TestAccuracy: 38.71%\n","Epoch: 7, lr=0.0010160639999999997, TrainLoss: 1.6329, TrainAccuracy: 41.73%, TestLoss: 1.8459, TestAccuracy: 39.44%\n","Epoch: 8, lr=0.0012192767999999997, TrainLoss: 1.6666, TrainAccuracy: 40.81%, TestLoss: 2.7768, TestAccuracy: 33.52%\n","Epoch: 9, lr=0.0008534937599999998, TrainLoss: 1.6036, TrainAccuracy: 42.64%, TestLoss: 1.6648, TestAccuracy: 42.37%\n","Epoch: 10, lr=0.0010241925119999996, TrainLoss: 1.6263, TrainAccuracy: 41.62%, TestLoss: 1.9040, TestAccuracy: 39.53%\n","Epoch: 11, lr=0.0007169347583999997, TrainLoss: 1.5739, TrainAccuracy: 43.81%, TestLoss: 1.6117, TestAccuracy: 43.68%\n","Stopping training in the epoch 11 as the average train loss 1.5739 is less than epsilon 1.6, acceptable 2LNN.\n","Training phase completes.\n","Regularization phase starts.\n","Epoch: 1, lr=0.001, TrainLoss: 1.5742, TrainAccuracy: 43.72%, TestLoss: 1.6304, TestAccuracy: 43.35%\n","Epoch: 2, lr=0.0012, TrainLoss: 1.6752, TrainAccuracy: 40.97%, TestLoss: 1.9554, TestAccuracy: 39.15%\n","Epoch: 3, lr=0.0008399999999999999, TrainLoss: 1.5886, TrainAccuracy: 43.07%, TestLoss: 1.8063, TestAccuracy: 40.83%\n","Epoch: 4, lr=0.0010079999999999998, TrainLoss: 1.6082, TrainAccuracy: 42.29%, TestLoss: 1.9400, TestAccuracy: 41.63%\n","Epoch: 5, lr=0.0007055999999999998, TrainLoss: 1.5692, TrainAccuracy: 43.69%, TestLoss: 1.6321, TestAccuracy: 44.47%\n","Epoch: 6, lr=0.0008467199999999997, TrainLoss: 1.5791, TrainAccuracy: 43.33%, TestLoss: 1.7327, TestAccuracy: 42.86%\n","Epoch: 7, lr=0.0005927039999999998, TrainLoss: 1.5440, TrainAccuracy: 44.46%, TestLoss: 1.6432, TestAccuracy: 44.01%\n","Stopping regularizing in the epoch 7 as the average train loss 1.5440 is less than epsilon 1.55, acceptable 2LNN.\n"]}],"source":["# Example usage\n","net, optimizer, device = reset_and_initialize()\n","criterion = nn.CrossEntropyLoss() #分類任務的損失函數\n","\n","trn_loss_eb_lg_ua = []\n","trn_acc_eb_lg_ua = []\n","val_loss_eb_lg_ua = []\n","val_acc_eb_lg_ua = []\n","\n","trn_loss_eb_lg_ua_r = []\n","trn_acc_eb_lg_ua_r = []\n","val_loss_eb_lg_ua_r = []\n","val_acc_eb_lg_ua_r = []  \n","\n","# Train the neural network\n","eta = 0.001\n","max_epochs = 10\n","epsilon = 1.6\n","epsilon_eta = 0.00001\n","last_loss = float('inf')\n","epoch_eb_lg_ua = 0\n","\n","eta_r = 0.001\n","max_epochs_r = 10\n","epsilon_r = 1.55\n","epsilon_eta_r = 0.00001\n","last_loss_r = float('inf')\n","epoch_eb_lg_ua_r = 0\n","\n","while True:\n","    net.train()\n","    running_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","    \n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss = running_loss / len(train_loader) #len(train_loader)\n","    train_accuracy = 100 * train_correct / train_total\n","    trn_loss_eb_lg_ua.append(avg_train_loss)\n","    trn_acc_eb_lg_ua.append(train_accuracy)\n","\n","    # 測試損失\n","    avg_test_loss, test_accuracy = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb_lg_ua+1}, lr={eta}, TrainLoss: {avg_train_loss:.4f}, TrainAccuracy: {train_accuracy:.2f}%, TestLoss: {avg_test_loss:.4f}, TestAccuracy: {test_accuracy:.2f}%\")\n","    val_loss_eb_lg_ua.append(avg_test_loss)\n","    val_acc_eb_lg_ua.append(test_accuracy)\n","    \n","    if avg_train_loss <= epsilon:\n","        print(f\"Stopping training in the epoch {epoch_eb_lg_ua+1} as the average train loss {avg_train_loss:.4f} is less than epsilon {epsilon}, acceptable 2LNN.\")\n","        break\n","    elif epoch_eb_lg_ua >= max_epochs:\n","        print(f\"Stopping training as the epoch {epoch_eb_lg_ua+1} is larger than  max epochs {max_epochs} , unacceptable 2LNN.\")\n","        break\n","    elif avg_train_loss < last_loss:\n","        eta *= 1.2\n","    else:\n","        eta *= 0.7\n","        if eta < epsilon_eta:\n","            print(f\"Stopping training  in the epoch {epoch_eb_lg_ua+1} as the eta {eta} is smaller than epsilon_eta {epsilon_eta}, unacceptable 2LNN.\")\n","            break\n","    \n","    # Update the optimizer's learning rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = eta\n","\n","    last_loss = avg_train_loss\n","    epoch_eb_lg_ua += 1\n","    \n","print('Training phase completes.')\n","# Save the acceptable state\n","torch.save(net.state_dict(), 'acceptable_2lnn.pth')\n","\n","# Load weights if needed and begin the regularization phase\n","net.load_state_dict(torch.load('acceptable_2lnn.pth'))\n","print(\"Regularization phase starts.\")\n","\n","# 保存初始正則化階段的最佳權重\n","best_weights_r = net.state_dict().copy()\n","\n","# Run a loop with custom loss that includes regularization\n","while True:\n","    net.train()\n","    running_loss_r = 0.0\n","    train_correct_r = 0\n","    train_total_r = 0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss_r += loss.item()\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_total_r += labels.size(0)\n","        train_correct_r += (predicted == labels).sum().item()\n","    \n","    # 訓練損失: 782個mini-batch loss的平均值\n","    avg_train_loss_r = running_loss_r / len(train_loader) #len(train_loader)\n","    train_accuracy_r = 100 * train_correct_r / train_total_r\n","    trn_loss_eb_lg_ua_r.append(avg_train_loss_r)\n","    trn_acc_eb_lg_ua_r.append(train_accuracy_r)\n","\n","    # 測試損失\n","    avg_test_loss_r, test_accuracy_r = TwoLayerNet.evaluate_model(net, test_loader, criterion)\n","    print(f\"Epoch: {epoch_eb_lg_ua_r+1}, lr={eta_r}, TrainLoss: {avg_train_loss_r:.4f}, TrainAccuracy: {train_accuracy_r:.2f}%, TestLoss: {avg_test_loss_r:.4f}, TestAccuracy: {test_accuracy_r:.2f}%\")\n","    val_loss_eb_lg_ua_r.append(avg_test_loss_r)\n","    val_acc_eb_lg_ua_r.append(test_accuracy_r)\n","    \n","    if avg_train_loss_r < epsilon_r:\n","        print(f\"Stopping regularizing in the epoch {epoch_eb_lg_ua_r+1} as the average train loss {avg_train_loss_r:.4f} is less than epsilon {epsilon_r}, acceptable 2LNN.\")\n","        break\n","    else:\n","        if epoch_eb_lg_ua_r >= max_epochs_r:\n","            net.load_state_dict(best_weights_r)\n","            print(f\"Stopping regularizing as the epoch {epoch_eb_lg_ua_r+1} is larger than  max epochs {max_epochs_r} , acceptable 2LNN.\")\n","            break\n","        elif avg_train_loss_r < last_loss_r:\n","            eta_r *= 1.2\n","            best_weights_r = net.state_dict().copy()\n","        else:\n","            eta_r *= 0.7\n","            if eta_r < epsilon_eta_r:\n","                net.load_state_dict(best_weights_r)\n","                print(f\"Stopping regularizing in the epoch {epoch_eb_lg_ua_r+1} as the eta {eta_r} is smaller than epsilon_eta {epsilon_eta_r}, acceptable 2LNN.\")\n","                break\n","    \n","    # Update the optimizer's learning rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = eta_r\n","\n","    last_loss_r = avg_train_loss_r\n","    epoch_eb_lg_ua_r += 1"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"17i_lVAcObw_63ta_ao69EgS0AUQzMMmu","timestamp":1709170399239}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
